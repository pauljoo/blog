---
title: k近邻算法
date: 2018-11-18 16:46:15
categories:
- 机器学习
tags:
---
k-近邻算法(kNN)

描述：采用测量不同特征值之间的距离方法进行分类。

优点:精度高、对异常值不敏感、无数据输入假定。

缺点:计算复杂度高、空间复杂度高。

适用数据范围:数值型和标称型

工作原理:我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个(通常k<20)最相似的数据。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

 

 

1.收集数据:可以使用任何方法。

2.准备数据:距离计算所需要的数值，最好是结构化的数据格式。

3.分析数据:可以使用任何方法。

4.训练算法:此步骤不适用于k-近邻算法。

5.测试算法:计算错误率。

6.使用算法:首先需要输入样本数据和结构化的输出结构，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算粗的分类执行后续的处理。

 

计算向量点的距离:

 

数值归一化

在处理不同取值范围的特征值时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。

 

特征值转化为0到1区间内的值:newValue =  (oldValue - min)/ (max - min)

 

k-近邻算法是分类数据最简单最有效的算法。k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。

k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。